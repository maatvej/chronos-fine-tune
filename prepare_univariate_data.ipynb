{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 1: Загрузка данных ---\n",
      "DataFrame успешно загружен. Форма: (14939782, 60)\n",
      "\n",
      "--- Шаг 2: Сохранение непрерывных временных рядов (на основе ClipID) в формате JSON Lines ---\n",
      "Найдено 182447 уникальных непрерывных рядов (ClipID).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd4cb5db8d4dc7881fd469b89bb220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Формирование и сохранение временных рядов:   0%|          | 0/182447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Готово! Данные для одномерного fine-tuning'а сохранены в: chronos_univariate_dataset\\univariate_train.jsonl\n",
      "Каждый ClipID был обработан как отдельный временной ряд.\n"
     ]
    }
   ],
   "source": [
    "# --- Импорт необходимых библиотек ---\n",
    "import pandas as pd  # Основная библиотека для работы с данными в формате таблиц (DataFrame)\n",
    "import json          # Библиотека для работы с форматом JSON, нужна для сохранения данных\n",
    "from pathlib import Path  # Удобный инструмент для работы с путями к файлам и папкам\n",
    "from tqdm.auto import tqdm  # Библиотека для создания красивых и информативных прогресс-баров\n",
    "import warnings      # Инструмент для управления предупреждениями Python\n",
    "\n",
    "# Отключаем предупреждения типа FutureWarning. Это делается для чистоты вывода,\n",
    "# так как некоторые библиотеки могут использовать устаревшие, но все еще работающие функции.\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# --- 1. КОНФИГУРАЦИЯ ---\n",
    "# В этом блоке мы определяем все ключевые параметры, чтобы легко менять их в одном месте.\n",
    "\n",
    "# Путь к исходному файлу с данными.\n",
    "INPUT_FILE = \"20250628_dm_by_clusters_train_preprocessed.parquet\"\n",
    "\n",
    "# Имя папки, куда будут сохранены подготовленные для обучения данные.\n",
    "OUTPUT_DIR = Path(\"chronos_univariate_dataset\")\n",
    "\n",
    "# Имя столбца, который является нашей целевой переменной (то, что мы хотим прогнозировать).\n",
    "TARGET = \"sales_qty\"\n",
    "\n",
    "# ВАЖНО: Минимальная длина временного ряда, который мы будем использовать для обучения.\n",
    "# ЗАЧЕМ ЭТО НУЖНО: Модель Chronos для обучения требует, чтобы у временного ряда была\n",
    "# достаточная длина для \"вырезания\" как минимум одного обучающего примера.\n",
    "# Обучающий пример состоит из `context` (история) и `prediction` (будущее).\n",
    "# Поэтому минимальная длина должна быть как минимум `context_length + prediction_length`.\n",
    "# Устанавливая здесь порог, мы заранее отфильтровываем слишком короткие ряды,\n",
    "# экономя время на их обработке и избегая ошибок на этапе обучения.\n",
    "MIN_SERIES_LENGTH = 102  # Например, для context=90, prediction=12 (90+12=102)\n",
    "\n",
    "\n",
    "# --- ОСНОВНОЙ КОД ---\n",
    "print(\"--- Шаг 1: Загрузка данных ---\")\n",
    "# Используем блок try-except для \"безопасной\" загрузки файла.\n",
    "# Если файл не будет найден, программа выведет понятное сообщение об ошибке и завершится.\n",
    "try:\n",
    "    # pd.read_parquet - это высокопроизводительный способ чтения данных из формата Parquet.\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "    print(f\"DataFrame успешно загружен. Форма: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Файл '{INPUT_FILE}' не найден. Проверьте путь и имя файла.\")\n",
    "    exit() # Завершаем выполнение скрипта, так как без данных он бесполезен.\n",
    "\n",
    "print(\"\\n--- Шаг 2: Сохранение непрерывных временных рядов (на основе ClipID) в формате JSON Lines ---\")\n",
    "# Создаем папку для выходных данных, если она еще не существует.\n",
    "# exist_ok=True предотвращает ошибку, если папка уже есть.\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "# Собираем полный путь к файлу, в который будем записывать данные.\n",
    "train_file = OUTPUT_DIR / \"univariate_train.jsonl\"\n",
    "\n",
    "# Мы определили, что в ваших данных каждый уникальный непрерывный\n",
    "# временной ряд идентифицируется тройкой: (product_id, cluster_id, clip_id).\n",
    "# Поэтому мы группируем DataFrame именно по этим трем столбцам.\n",
    "# Каждая 'group' в цикле ниже будет представлять собой один полный, непрерывный временной ряд.\n",
    "grouped = df.groupby(['product_id', 'cluster_id', 'clip_id'])\n",
    "\n",
    "print(f\"Найдено {len(grouped)} уникальных непрерывных рядов (ClipID).\")\n",
    "\n",
    "# Открываем файл для записи. `with open(...)` гарантирует, что файл будет корректно закрыт\n",
    "# даже в случае ошибки.\n",
    "with open(train_file, 'w') as fp:\n",
    "    # Итерируемся по каждой группе. `tqdm` оборачивает итератор, чтобы показать прогресс-бар.\n",
    "    # `name` будет кортежем (product_id, cluster_id, clip_id), `group` - DataFrame с данными для этого ряда.\n",
    "    for name, group in tqdm(grouped, desc=\"Формирование и сохранение временных рядов\"):\n",
    "        \n",
    "        # На всякий случай сортируем данные внутри группы по временному индексу.\n",
    "        # Это гарантирует, что последовательность данных корректна.\n",
    "        group = group.sort_values('time_idx')\n",
    "        \n",
    "        # Извлекаем значения целевой переменной и преобразуем их в обычный список Python.\n",
    "        target = group[TARGET].tolist()\n",
    "        \n",
    "        # --- ФИЛЬТРАЦИЯ ---\n",
    "        # Проверяем, соответствует ли длина текущего непрерывного ряда нашему порогу.\n",
    "        if len(target) < MIN_SERIES_LENGTH:\n",
    "            continue # `continue` немедленно переходит к следующей итерации цикла, пропуская этот ряд.\n",
    "\n",
    "        # --- ФОРМИРОВАНИЕ УНИКАЛЬНОГО ID И ЗАПИСИ ---\n",
    "        # Распаковываем кортеж `name`, чтобы получить отдельные ID.\n",
    "        product_id, cluster_id, clip_id = name\n",
    "        # Создаем уникальный идентификатор для этого ряда, объединяя все три ID.\n",
    "        # Это полезно для последующей отладки и анализа результатов.\n",
    "        item_id = f\"{product_id}_{cluster_id}_{clip_id}\"\n",
    "\n",
    "        # Создаем словарь - одну запись для нашего JSON Lines файла.\n",
    "        # Формат ({'start': ..., 'target': ..., 'item_id': ...}) стандартен для библиотеки GluonTS,\n",
    "        # на которой основан наш скрипт обучения.\n",
    "        entry = {\n",
    "            \"start\": 0,          # ПОЧЕМУ 0: Мы используем фиктивную числовую дату начала.\n",
    "                                 # Это решает проблему совместимости с GluonTS, который\n",
    "                                 # пытается парсить строки как даты, но для математических\n",
    "                                 # операций (вычисления смещений) ему нужно число.\n",
    "            \"target\": target,    # Список с данными о продажах.\n",
    "            \"item_id\": item_id   # Уникальный идентификатор ряда.\n",
    "        }\n",
    "        \n",
    "        # Преобразуем словарь в строку JSON и записываем ее в файл, добавляя символ переноса строки.\n",
    "        fp.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"\\nГотово! Данные для одномерного fine-tuning'а сохранены в: {train_file}\")\n",
    "print(\"Каждый ClipID был обработан как отдельный временной ряд.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
